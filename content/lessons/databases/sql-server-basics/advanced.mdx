# SQL Server Basics - Advanced

Master SQL Server performance, monitoring, and enterprise integration patterns.

---

## Section 1: Introduction to SQL Server

### SQL Server Internals

Understanding how SQL Server stores and manages data:

#### Pages and Extents

- **Page** - 8KB unit of storage (smallest unit SQL Server reads/writes)
- **Extent** - 8 contiguous pages (64KB)
- **Uniform extent** - All pages belong to one object
- **Mixed extent** - Pages shared by multiple small objects

```sql
-- View page allocation
DBCC IND('MyAppDb', 'Users', -1);

-- View page contents (use with caution!)
DBCC TRACEON(3604);
DBCC PAGE('MyAppDb', 1, 152, 3);
```

#### Buffer Pool and Memory Management

```sql
-- Check buffer pool usage
SELECT 
    DB_NAME(database_id) AS DatabaseName,
    COUNT(*) * 8 / 1024 AS BufferPoolMB
FROM sys.dm_os_buffer_descriptors
WHERE database_id > 4
GROUP BY database_id
ORDER BY BufferPoolMB DESC;

-- Memory clerks
SELECT 
    type,
    SUM(pages_kb) / 1024 AS MemoryMB
FROM sys.dm_os_memory_clerks
GROUP BY type
ORDER BY MemoryMB DESC;
```

<ProgressCheckpoint section="intro-sql-server" xpReward={18} />

---

## Section 2: SQL Server Management Studio (SSMS)

### Extended Events

Modern replacement for SQL Profiler:

```sql
-- Create Extended Event session
CREATE EVENT SESSION [QueryPerformance]
ON SERVER
ADD EVENT sqlserver.sql_statement_completed
(
    ACTION(sqlserver.sql_text, sqlserver.database_name)
    WHERE duration > 1000000  -- 1 second in microseconds
)
ADD TARGET package0.event_file
(
    SET filename = N'C:\Temp\QueryPerformance.xel'
);

-- Start the session
ALTER EVENT SESSION [QueryPerformance]
ON SERVER STATE = START;

-- Query the results
SELECT 
    event_data.value('(event/@timestamp)[1]', 'datetime2') AS EventTime,
    event_data.value('(event/data[@name="duration"]/value)[1]', 'bigint') / 1000 AS DurationMS,
    event_data.value('(event/action[@name="sql_text"]/value)[1]', 'nvarchar(max)') AS QueryText
FROM (
    SELECT CAST(event_data AS XML) AS event_data
    FROM sys.fn_xe_file_target_read_file('C:\Temp\QueryPerformance*.xel', null, null, null)
) AS events;
```

### Dynamic Management Views (DMVs)

```sql
-- Currently executing queries
SELECT 
    session_id,
    status,
    command,
    cpu_time,
    total_elapsed_time,
    reads,
    writes,
    SUBSTRING(st.text, 
        (er.statement_start_offset / 2) + 1,
        ((CASE er.statement_end_offset
            WHEN -1 THEN DATALENGTH(st.text)
            ELSE er.statement_end_offset
        END - er.statement_start_offset) / 2) + 1
    ) AS QueryText
FROM sys.dm_exec_requests er
CROSS APPLY sys.dm_exec_sql_text(er.sql_handle) st
WHERE session_id > 50;

-- Blocking chains
SELECT 
    blocking_session_id,
    session_id,
    wait_type,
    wait_time,
    wait_resource
FROM sys.dm_exec_requests
WHERE blocking_session_id <> 0;
```

<ProgressCheckpoint section="ssms-basics" xpReward={19} />

---

## Section 3: T-SQL Features

###  Query Execution Plans

Execution plans show exactly how SQL Server runs your queries:

<DotnetCodePreview
  title="Analyzing Execution Plans"
  code={`-- Enable actual execution plan
SET STATISTICS IO ON;
SET STATISTICS TIME ON;

-- Run your query
SELECT u.Name, COUNT(o.Id) AS OrderCount
FROM Users u
LEFT JOIN Orders o ON u.Id = o.UserId
WHERE u.CreatedAt > '2024-01-01'
GROUP BY u.Id, u.Name;

-- Key metrics to watch:
-- Logical reads: How many 8KB pages read from cache
-- Physical reads: Pages read from disk (slow!)
-- Elapsed time: Total execution time`}
  steps={[
    {
      lineNumbers: [2, 3],
      highlight: "Statistics",
      explanation: "Shows I/O and timing info in the Messages tab"
    },
    {
      lineNumbers: [12, 13, 14],
      highlight: "Key metrics",
      explanation: "Lower logical reads = better. Physical reads = cache miss"
    }
  ]}
/>

### Understanding Plan Operators

| Operator | Good/Bad | Meaning |
|:---------|:---------|:--------|
| Index Seek |  Good | Directly jumps to matching rows |
| Index Scan | ️ Depends | Reads entire index |
| Table Scan |  Usually Bad | Reads entire table |
| Key Lookup | ️ Expensive | Extra lookup for non-covered columns |
| Sort | ️ Expensive | Must sort in memory/tempdb |

##  Performance Tuning

### Identifying Slow Queries

```sql
-- Find top 10 most CPU-intensive queries
SELECT TOP 10
    qs.total_worker_time / qs.execution_count AS AvgCPU,
    qs.execution_count,
    SUBSTRING(st.text, 
        (qs.statement_start_offset / 2) + 1,
        ((CASE qs.statement_end_offset
            WHEN -1 THEN DATALENGTH(st.text)
            ELSE qs.statement_end_offset
        END - qs.statement_start_offset) / 2) + 1
    ) AS QueryText
FROM sys.dm_exec_query_stats qs
CROSS APPLY sys.dm_exec_sql_text(qs.sql_handle) st
ORDER BY AvgCPU DESC;
```

### Index Recommendations

```sql
-- Missing indexes SQL Server suggests
SELECT 
    d.statement AS TableName,
    d.equality_columns,
    d.inequality_columns,
    d.included_columns,
    s.avg_user_impact,
    'CREATE INDEX IX_' + REPLACE(d.statement, '.', '_') + 
        ' ON ' + d.statement + 
        ' (' + ISNULL(d.equality_columns, '') + 
        ISNULL(d.inequality_columns, '') + ')' +
        ISNULL(' INCLUDE (' + d.included_columns + ')', '') AS SuggestedIndex
FROM sys.dm_db_missing_index_details d
JOIN sys.dm_db_missing_index_groups g ON d.index_handle = g.index_handle
JOIN sys.dm_db_missing_index_group_stats s ON g.index_group_handle = s.group_handle
WHERE d.database_id = DB_ID()
ORDER BY s.avg_user_impact DESC;
```

##  Table Partitioning

For very large tables, partitioning splits data across filegroups:

<DotnetCodePreview
  title="Table Partitioning Setup"
  code={`-- 1. Create partition function (how to split)
CREATE PARTITION FUNCTION pf_OrderDate (datetime2)
AS RANGE RIGHT FOR VALUES 
    ('2022-01-01', '2023-01-01', '2024-01-01', '2025-01-01');

-- 2. Create partition scheme (where to store)
CREATE PARTITION SCHEME ps_OrderDate
AS PARTITION pf_OrderDate
ALL TO ([PRIMARY]);

-- 3. Create partitioned table
CREATE TABLE Orders_Partitioned (
    Id INT IDENTITY(1,1),
    OrderDate DATETIME2 NOT NULL,
    Total DECIMAL(18,2),
    -- ... other columns
) ON ps_OrderDate(OrderDate);`}
  steps={[
    {
      lineNumbers: [2, 3, 4],
      highlight: "Partition function",
      explanation: "RANGE RIGHT means values go in the partition to their right"
    },
    {
      lineNumbers: [7, 8, 9],
      highlight: "Partition scheme",
      explanation: "Maps partitions to filegroups - can spread across disks"
    },
    {
      lineNumbers: [16],
      highlight: "ON clause",
      explanation: "Creates the table on the partition scheme using OrderDate"
    }
  ]}
/>

### Benefits of Partitioning:
- Faster queries on recent data
- Easy archival of old data
- Parallel query processing
- Partition-level maintenance

##  Row-Level Security

Control data access at the row level:

<DotnetCodePreview
  title="Row-Level Security"
  code={`-- 1. Create a security predicate function
CREATE FUNCTION fn_SecurityPredicate(@UserId INT)
RETURNS TABLE
WITH SCHEMABINDING
AS
    RETURN SELECT 1 AS AccessAllowed
    WHERE @UserId = CAST(SESSION_CONTEXT(N'UserId') AS INT);

-- 2. Create security policy
CREATE SECURITY POLICY OrdersSecurityPolicy
ADD FILTER PREDICATE dbo.fn_SecurityPredicate(UserId)
ON dbo.Orders
WITH (STATE = ON);

-- 3. Set context in your app
EXEC sp_set_session_context @key = N'UserId', @value = 42;

-- Now queries automatically filter by user!
SELECT * FROM Orders; -- Only sees UserId = 42`}
  steps={[
    {
      lineNumbers: [2, 3, 4, 5, 6, 7],
      highlight: "Security function",
      explanation: "Returns rows only if UserId matches session context"
    },
    {
      lineNumbers: [10, 11, 12, 13],
      highlight: "Security policy",
      explanation: "Applies the function as a filter on the Orders table"
    },
    {
      lineNumbers: [16],
      highlight: "Session context",
      explanation: "App sets this per-request - database enforces access"
    }
  ]}
/>

##  In-Memory OLTP

Extreme performance for high-throughput scenarios:

```sql
-- 1. Create memory-optimized table
CREATE TABLE Orders_InMemory (
    Id INT IDENTITY(1,1) PRIMARY KEY NONCLUSTERED,
    UserId INT NOT NULL,
    Total DECIMAL(18,2) NOT NULL,
    CreatedAt DATETIME2 NOT NULL DEFAULT GETDATE()
) WITH (
    MEMORY_OPTIMIZED = ON,
    DURABILITY = SCHEMA_AND_DATA
);

-- 2. Native compiled stored procedure
CREATE PROCEDURE sp_InsertOrder_Native
    @UserId INT,
    @Total DECIMAL(18,2)
WITH NATIVE_COMPILATION, SCHEMABINDING
AS
BEGIN ATOMIC WITH (
    TRANSACTION ISOLATION LEVEL = SNAPSHOT,
    LANGUAGE = N'English'
)
    INSERT INTO dbo.Orders_InMemory (UserId, Total)
    VALUES (@UserId, @Total);
END;
```

##  Advanced .NET Integration

### Connection Resiliency

```csharp
// In Program.cs with Entity Framework Core
builder.Services.AddDbContext<AppDbContext>(options =>
    options.UseSqlServer(
        connectionString,
        sqlOptions =>
        {
            sqlOptions.EnableRetryOnFailure(
                maxRetryCount: 5,
                maxRetryDelay: TimeSpan.FromSeconds(30),
                errorNumbersToAdd: null
            );
            sqlOptions.CommandTimeout(60);
        }
    )
);
```

### Bulk Operations

```csharp
// Using SqlBulkCopy for fast inserts
using var bulkCopy = new SqlBulkCopy(connection);
bulkCopy.DestinationTableName = "Orders";
bulkCopy.BatchSize = 10000;

// Map columns
bulkCopy.ColumnMappings.Add("UserId", "UserId");
bulkCopy.ColumnMappings.Add("Total", "Total");

// Bulk insert from DataTable or IDataReader
await bulkCopy.WriteToServerAsync(dataTable);
```

<SqlServerExplorer mode="advanced" />

## Performance Checklist

| Area | Action |
|:-----|:-------|
| Indexes | Create based on query patterns |
| Statistics | Keep up-to-date with regular maintenance |
| Query Plans | Review for scans and key lookups |
| Blocking | Monitor with dm_exec_requests |
| Memory | Check for memory grants and spills |
| TempDB | Monitor for contention |

<ProgressCheckpoint section="tsql-features" xpReward={19} />

---

## Section 4: .NET Integration

### Entity Framework Core with SQL Server

```csharp
// DbContext configuration
public class AppDbContext : DbContext
{
    public AppDbContext(DbContextOptions<AppDbContext> options)
        : base(options)
    {
    }

    public DbSet<User> Users { get; set; }
    public DbSet<Order> Orders { get; set; }

    protected override void OnModelCreating(ModelBuilder modelBuilder)
    {
        // SQL Server specific configurations
        modelBuilder.Entity<User>(entity =>
        {
            entity.ToTable("Users");
            entity.HasKey(e => e.Id);
            
            entity.Property(e => e.Id)
                .UseIdentityColumn(1, 1);  // IDENTITY(1,1)
            
            entity.Property(e => e.Name)
                .HasMaxLength(100)
                .IsUnicode(true);  // NVARCHAR
            
            entity.Property(e => e.Balance)
                .HasColumnType("decimal(18,2)");
            
            entity.Property(e => e.CreatedAt)
                .HasColumnType("datetime2")
                .HasDefaultValueSql("GETDATE()");
            
            // Index
            entity.HasIndex(e => e.Email)
                .IsUnique()
                .HasDatabaseName("IX_Users_Email");
        });

        // Relationship
        modelBuilder.Entity<Order>()
            .HasOne(o => o.User)
            .WithMany(u => u.Orders)
            .HasForeignKey(o => o.UserId)
            .OnDelete(DeleteBehavior.Cascade);
    }
}
```

### Raw SQL and Stored Procedures with EF Core

```csharp
// Raw SQL query
var users = await context.Users
    .FromSqlRaw("SELECT * FROM Users WHERE CreatedAt > {0}", startDate)
    .ToListAsync();

// Stored procedure
var orders = await context.Orders
    .FromSqlRaw("EXEC GetUserOrders @UserId = {0}, @MinTotal = {1}", 
        userId, minTotal)
    .ToListAsync();

// Execute non-query
await context.Database.ExecuteSqlRawAsync(
    "UPDATE Users SET LastLoginDate = GETDATE() WHERE Id = {0}", 
    userId);
```

### Advanced Connection Resiliency

```csharp
builder.Services.AddDbContext<AppDbContext>(options =>
{
    options.UseSqlServer(
        connectionString,
        sqlOptions =>
        {
            // Retry on transient failures
            sqlOptions.EnableRetryOnFailure(
                maxRetryCount: 5,
                maxRetryDelay: TimeSpan.FromSeconds(30),
                errorNumbersToAdd: new[] { 4060, 40197, 40501, 40613, 49918, 49919, 49920 }
            );
            
            // Command timeout
            sqlOptions.CommandTimeout(60);
            
            // Use row versioning for optimistic concurrency
            sqlOptions.UseQuerySplittingBehavior(QuerySplittingBehavior.SplitQuery);
            
            // Migrations assembly
            sqlOptions.MigrationsAssembly("MyApp.Data");
        }
    );
    
    // Enable sensitive data logging in development
    if (builder.Environment.IsDevelopment())
    {
        options.EnableSensitiveDataLogging();
        options.EnableDetailedErrors();
    }
});
```

### Bulk Operations with EFCore.BulkExtensions

```csharp
// Install: EFCore.BulkExtensions
using EFCore.BulkExtensions;

// Bulk insert (much faster than SaveChanges)
var users = GenerateUsers(10000);
await context.BulkInsertAsync(users);

// Bulk update
await context.BulkUpdateAsync(users);

// Bulk delete
await context.BulkDeleteAsync(users);

// Bulk insert or update
await context.BulkInsertOrUpdateAsync(users);
```

### Temporal Tables with EF Core

```csharp
// Configure temporal table
modelBuilder.Entity<User>(entity =>
{
    entity.ToTable("Users", tb => tb.IsTemporal(ttb =>
    {
        ttb.HasPeriodStart("ValidFrom");
        ttb.HasPeriodEnd("ValidTo");
        ttb.UseHistoryTable("UsersHistory");
    }));
});

// Query historical data
var historicalUsers = await context.Users
    .TemporalAsOf(DateTime.UtcNow.AddDays(-30))
    .Where(u => u.Id == userId)
    .ToListAsync();

// Query all history
var allHistory = await context.Users
    .TemporalAll()
    .Where(u => u.Id == userId)
    .OrderBy(u => EF.Property<DateTime>(u, "ValidFrom"))
    .ToListAsync();
```

### Performance Monitoring

```csharp
// Log slow queries
builder.Services.AddDbContext<AppDbContext>(options =>
{
    options.UseSqlServer(connectionString)
        .LogTo(
            message => Console.WriteLine(message),
            new[] { DbLoggerCategory.Database.Command.Name },
            LogLevel.Information,
            DbContextLoggerOptions.SingleLine | DbContextLoggerOptions.UtcTime
        );
});

// Custom interceptor for query logging
public class QueryLoggingInterceptor : DbCommandInterceptor
{
    private readonly ILogger<QueryLoggingInterceptor> _logger;

    public QueryLoggingInterceptor(ILogger<QueryLoggingInterceptor> logger)
    {
        _logger = logger;
    }

    public override async ValueTask<DbDataReader> ReaderExecutedAsync(
        DbCommand command,
        CommandExecutedEventData eventData,
        DbDataReader result,
        CancellationToken cancellationToken = default)
    {
        if (eventData.Duration.TotalMilliseconds > 1000)
        {
            _logger.LogWarning(
                "Slow query detected: {Duration}ms - {CommandText}",
                eventData.Duration.TotalMilliseconds,
                command.CommandText);
        }

        return await base.ReaderExecutedAsync(command, eventData, result, cancellationToken);
    }
}

// Register interceptor
builder.Services.AddDbContext<AppDbContext>(options =>
{
    options.UseSqlServer(connectionString)
        .AddInterceptors(new QueryLoggingInterceptor(logger));
});
```

<ProgressCheckpoint section="dotnet-integration" xpReward={19} />
